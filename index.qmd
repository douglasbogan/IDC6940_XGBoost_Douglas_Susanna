---
title: "Capstone Projects for Data Science - Spring 2026"
subtitle: "XGBoost Methodology"
author: "Douglas Bogan (Advisor: Dr. Cohen)"
date: '`r Sys.Date()`'
format:
  html:
    code-fold: true
course: Capstone Projects in Data Science
bibliography: references.bib # file contains bibtex for references
#always_allow_html: true # this allows to get PDF with HTML features
self-contained: true
execute: 
  warning: false
  message: false
editor: 
  markdown: 
    wrap: 72
---

Slides: [slides.html](slides.html){target="_blank"} ( Go to `slides.qmd`
to edit)

::: callout-important
**Remember:** Your goal is to make your audience understand and care
about your findings. By crafting a compelling story, you can effectively
communicate the value of your data science project.

Carefully read this template since it has instructions and tips to
writing!

Nice report!
:::

## Introduction

The introduction should:

-   Develop a storyline that captures attention and maintains interest.

-   Your audience is your peers

-   Clearly state the problem or question you're addressing.

<!-- -->

-   Introduce why it is relevant needs.

-   Provide an overview of your approach.


**Summary of Research Article 1 with Citation:**

*Experimenting XGBoost algorithm for prediction and classification of different datasets:*

This research paper gives an analysis of different advantages of XGBoost as a machine learning methodology. XGBoost is presented in this research paper as a faster, more streamlined progression of traditional Gradient Boosting, especially well‑suited for structured machine learning tasks. Across four datasets which include two classification datasets and two regression datasets, the authors show that XGBoost often matches or exceeds Gradient Boosting’s accuracy on classification problems and delivers comparable but sometimes more variable results on regression. Its biggest advantage is speed: by redesigning how trees are built and processed, XGBoost trains on data dramatically faster while still producing competitive models. The study also highlights how XGBoost’s feature‑importance outputs help interpret which variables drive predictions. The authors conclude that although XGBoost isn’t guaranteed to always be more accurate, its efficiency and practical design make it a strong choice for many real-world problems. (@ramraj2016experimenting)

**Summary of Research Article 2 with Citation:**

*Comparative analysis of advanced models for predicting housing prices: a review:*

This review compares traditional hedonic price models (HPM) with machine learning approaches for predicting housing prices and finds a clear complementarity effect between HPM and machine learning models, such as XGBoost. HPMs remain an important tool for interpretation, revealing which property attributes like location, size, neighborhood features, and distance to centers drive value, while machine learning models deliver superior predictive accuracy by capturing non‑linear relationships and complex interactions that linear regressions miss. Tree‑based and ensemble methods dominate recent studies. Random Forest is the most frequently used model, and gradient‑boosted algorithms such as XGBoost and LightGBM often achieve the highest fit and the lowest error metrics (higher R^2, lower RMSE/MAPE) when regional or cluster variables are included. XGBoost in particular excels at handling large datasets, modeling non‑linear effects from amenities and spatial clusters, and improving fit when economic or geographic clusters are added. In several comparative studies it produced the best balance of high R^2 and low absolute error. The paper also highlights that ensembles and hybrid approaches typically outperform single models, and that predictive gains depend heavily on the quality and breadth of input variables. External contextual features (transport access, neighborhood socioeconomic indicators) materially improve results. Therefore, if seeking both interpretability and accuracy, the research paper suggests pairing HPM for explanation with tree‑based and/or boosted ML models (notably XGBoost) for prediction. This combination yields robust insight into value drivers while tightening forecasts. In conclusion, XGBoost is a strong contender as a machine learning model for house price predictions. (@moreno2025comparative)

**Summary of Research Article 3 with Citation:**

*A Simple and Fast Baseline for Tuning Large XGBoost Models*

This research paper shows a simple, practical method to make tuning XGBoost much faster on very large tables of data: train and test on small, uniformly sampled slices of the dataset, then use those results to pick promising settings before training on the full data. The goal was to speed up hyperparameter tuning for XGBoost when datasets are huge and full training is slow. The authors of this paper found three surprising and useful things. For one, training time scales almost linearly with dataset size (so a 10% slice trains about 10× faster). Secondly, good hyperparameter choices on small slices tend to stay good on the full data (the ranking of configurations is preserved). And third, tuning on as little as 1% of the data often finds settings that, after retraining on the full set, perform nearly as well (average gap falls from ~3.3% to ~1.4%). They tested this across multiple 15–70 GB tabular datasets and showed that resource‑aware search methods like Hyperband (and Hyperband combined with Bayesian optimization) find strong models far faster than a blind random search.
Further, the paper highlights that XGBoost continues to excel on large tabular problems. It’s scalable, robust, and surprisingly tolerant of being tuned on small, representative samples. The main limitation is that uniform subsampling assumes the data are independent, identically distributed, and representative. If the dataset has strong biases or rare subgroups, simple uniform sampling can miss them and hurt results. The authors suggest future work on smarter sampling strategies to make the approach more reliable in those cases. Overall, the takeaway is practical and optimistic: a very easy baseline of uniform subsampling plus multi‑fidelity tuning gives big speedups with only modest tradeoffs in accuracy, and reveals unexpectedly favorable behavior of XGBoost on large, real‑world tabular data. (@kapoor2021simple)

**Summary of Research Article 4 with Citation:**

*Prediction of House Price Using XGBoost Regression Algorithm*

This research paper sets out to build a practical model that predicts house sale prices so buyers and sellers can make better decisions. The authors show that this problem matters because prices change with location, neighborhood and amenities. The author also explains how they cleaned and prepared a Kaggle dataset (about 80 features and ~1,500 records) using standard steps like filling missing values, removing outliers, and converting categories with one hot encoding. They then train an XGBoost regression model and compare different train/validation/test splits. The best split (80/10/10) produced the lowest test error (about 4.4%). The paper concludes that XGBoost improves prediction accuracy and reduces investment risk for customers. It also notes limits like that house prices vary by region and more features or deeper models could further reduce error and broaden applicability. The authors recommend expanding the database to include more cities and exploring deeper learning methods as future work. (@avanijaa2021prediction)



## Methods

-   Detail the models or algorithms used.

-   Justify your choices based on the problem and data.

## Analysis and Results

### Data Exploration and Visualization

-   Describe your data sources and collection process.

-   Present initial findings and insights through visualizations.

-   Highlight unexpected patterns or anomalies.



```

### Modeling and Results

-   Explain your data preprocessing and cleaning steps.

-   Present your key findings in a clear and concise manner.

-   Use visuals to support your claims.

-   **Tell a story about what the data reveals.**


### Conclusion

-   Summarize your key findings.

-   Discuss the implications of your results.

## References

